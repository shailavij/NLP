{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentence_Embedding_Feb_2021.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPwlZPPEDMyLbVjKiucN1wt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shailavij/NLP/blob/master/Sentence_Embedding_Feb_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U79H5hQnm6_T",
        "outputId": "8ec66c8d-ba19-4b5b-9b6e-a9d9f2957c02"
      },
      "source": [
        "\r\n",
        "!pip install sentence-transformers\r\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.6/dist-packages (0.4.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.41.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.19.5)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.1.95)\n",
            "Requirement already satisfied: transformers<5.0.0,>=3.1.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.3.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (0.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.0.43)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.10.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.9)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dBxLVey48KN",
        "outputId": "68e00ba7-c0a6-4ed2-d02a-5686c1710a36"
      },
      "source": [
        "! pip install elasticsearch[async]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting elasticsearch[async]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/68/76c5d46cc6a48fddb759f585bc8728caa11bfc9b812ce6705fc5f99beab2/elasticsearch-7.11.0-py2.py3-none-any.whl (325kB)\n",
            "\u001b[K     |████████████████████████████████| 327kB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from elasticsearch[async]) (2020.12.5)\n",
            "Requirement already satisfied: urllib3<2,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from elasticsearch[async]) (1.24.3)\n",
            "Collecting aiohttp<4,>=3; extra == \"async\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/e6/d4b6235d776c9b33f853e603efede5aac5a34f71ca9d3877adb30492eb4e/aiohttp-3.7.3-cp36-cp36m-manylinux2014_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 7.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp<4,>=3; extra == \"async\"->elasticsearch[async]) (3.7.4.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp<4,>=3; extra == \"async\"->elasticsearch[async]) (20.3.0)\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp<4,>=3; extra == \"async\"->elasticsearch[async]) (3.0.4)\n",
            "Collecting idna-ssl>=1.0; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/46/03/07c4894aae38b0de52b52586b24bf189bb83e4ddabfe2e2c8f2419eec6f4/idna-ssl-1.1.0.tar.gz\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Collecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/08/52b26b44bce7b818b410aee37c5e424c9ea420c557bca97dc2adac29b151/yarl-1.6.3-cp36-cp36m-manylinux2014_x86_64.whl (293kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 15.9MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a1/35/b22524d6b9cacfb4c5eff413a069bbc17c6ea628e54da5c6c989998ced5f/multidict-5.1.0-cp36-cp36m-manylinux2014_x86_64.whl (141kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 18.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna>=2.0 in /usr/local/lib/python3.6/dist-packages (from idna-ssl>=1.0; python_version < \"3.7\"->aiohttp<4,>=3; extra == \"async\"->elasticsearch[async]) (2.10)\n",
            "Building wheels for collected packages: idna-ssl\n",
            "  Building wheel for idna-ssl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-cp36-none-any.whl size=3163 sha256=ed0ac2eae7eae6e507ef0bea8c4d606bbef361542458d8982d9108a6c8977917\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/00/b3/32d613e19e08a739751dd6bf998cfed277728f8b2127ad4eb7\n",
            "Successfully built idna-ssl\n",
            "Installing collected packages: idna-ssl, async-timeout, multidict, yarl, aiohttp, elasticsearch\n",
            "Successfully installed aiohttp-3.7.3 async-timeout-3.0.1 elasticsearch-7.11.0 idna-ssl-1.1.0 multidict-5.1.0 yarl-1.6.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lH_eWY4c5-Q",
        "outputId": "02dec981-ac1b-41d9-d44f-6c46b67e3322"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\r\n",
        "model = SentenceTransformer('paraphrase-distilroberta-base-v1')\r\n",
        "\r\n",
        "#Our sentences we like to encode\r\n",
        "sentences = ['This framework generates embeddings for each input sentence',\r\n",
        "    'Sentences are passed as a list of string.',\r\n",
        "    'The quick brown fox jumps over the lazy dog.']\r\n",
        "\r\n",
        "#Sentences are encoded by calling model.encode()\r\n",
        "embeddings = model.encode(sentences)\r\n",
        "\r\n",
        "#Print the embeddings\r\n",
        "for sentence, embedding in zip(sentences, embeddings):\r\n",
        "    print(\"Sentence:\", sentence)\r\n",
        "    #print(\"Embedding:\", embedding)\r\n",
        "    print(\"embeddings_size\",embeddings.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence: This framework generates embeddings for each input sentence\n",
            "embeddings_size (3, 768)\n",
            "Sentence: Sentences are passed as a list of string.\n",
            "embeddings_size (3, 768)\n",
            "Sentence: The quick brown fox jumps over the lazy dog.\n",
            "embeddings_size (3, 768)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYfy_WvsjDSB",
        "outputId": "dfcf6668-1869-44f4-a753-c1e31c7c4052"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer, util\r\n",
        "model = SentenceTransformer('paraphrase-distilroberta-base-v1')\r\n",
        "\r\n",
        "#Sentences are encoded by calling model.encode()\r\n",
        "emb1 = model.encode(\"This is a red cat with a hat.\")\r\n",
        "emb2 = model.encode(\"Have you seen my red cat?\")\r\n",
        "\r\n",
        "cos_sim = util.pytorch_cos_sim(emb1, emb2)\r\n",
        "print(\"Cosine-Similarity:\", cos_sim)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cosine-Similarity: tensor([[0.5625]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah4DIvP-nXjX",
        "outputId": "72344310-a4d6-4c82-9a8e-4a5d60b85dac"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer, util\r\n",
        "model = SentenceTransformer('paraphrase-distilroberta-base-v1')\r\n",
        "\r\n",
        "# Two lists of sentences\r\n",
        "sentences1 = ['The cat sits outside',\r\n",
        "             'A man is playing guitar',\r\n",
        "             'The new movie is awesome']\r\n",
        "\r\n",
        "sentences2 = ['The dog plays in the garden',\r\n",
        "              'A woman watches TV',\r\n",
        "              'The new movie is so great']\r\n",
        "\r\n",
        "#Compute embedding for both lists\r\n",
        "embeddings1 = model.encode(sentences1, convert_to_tensor=True)\r\n",
        "embeddings2 = model.encode(sentences2, convert_to_tensor=True)\r\n",
        "\r\n",
        "#Compute cosine-similarits\r\n",
        "cosine_scores = util.pytorch_cos_sim(embeddings1, embeddings2)\r\n",
        "\r\n",
        "#Output the pairs with their score\r\n",
        "for i in range(len(sentences1)):\r\n",
        "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The cat sits outside \t\t The dog plays in the garden \t\t Score: 0.4579\n",
            "A man is playing guitar \t\t A woman watches TV \t\t Score: 0.1759\n",
            "The new movie is awesome \t\t The new movie is so great \t\t Score: 0.9283\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfDe2vDXnnOe"
      },
      "source": [
        " #find out the pairs with the highest cosine similarity scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZUJo1z_qnFNz",
        "outputId": "c737360a-6d79-4051-f937-2de1e54b4218"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer, util\r\n",
        "\r\n",
        "model = SentenceTransformer('paraphrase-distilroberta-base-v1')\r\n",
        "\r\n",
        "# Single list of sentences\r\n",
        "sentences = ['The cat sits outside',\r\n",
        "             'A man is playing guitar',\r\n",
        "             'I love pasta',\r\n",
        "             'The new movie is awesome',\r\n",
        "             'The cat plays in the garden',\r\n",
        "             'A woman watches TV',\r\n",
        "             'The new movie is so great',\r\n",
        "             'Do you like pizza?']\r\n",
        "\r\n",
        "#Compute embeddings\r\n",
        "embeddings = model.encode(sentences, convert_to_tensor=True)\r\n",
        "\r\n",
        "#Compute cosine-similarities for each sentence with each other sentence\r\n",
        "cosine_scores = util.pytorch_cos_sim(embeddings, embeddings)\r\n",
        "\r\n",
        "#Find the pairs with the highest cosine similarity scores\r\n",
        "pairs = []\r\n",
        "for i in range(len(cosine_scores)-1):\r\n",
        "    for j in range(i+1, len(cosine_scores)):\r\n",
        "        pairs.append({'index': [i, j], 'score': cosine_scores[i][j]})\r\n",
        "\r\n",
        "#Sort scores in decreasing order\r\n",
        "pairs = sorted(pairs, key=lambda x: x['score'], reverse=True)\r\n",
        "\r\n",
        "for pair in pairs[0:10]:\r\n",
        "    i, j = pair['index']\r\n",
        "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[i], sentences[j], pair['score']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The new movie is awesome \t\t The new movie is so great \t\t Score: 0.9283\n",
            "The cat sits outside \t\t The cat plays in the garden \t\t Score: 0.6855\n",
            "I love pasta \t\t Do you like pizza? \t\t Score: 0.5420\n",
            "I love pasta \t\t The new movie is awesome \t\t Score: 0.2629\n",
            "I love pasta \t\t The new movie is so great \t\t Score: 0.2268\n",
            "The new movie is awesome \t\t Do you like pizza? \t\t Score: 0.1885\n",
            "A man is playing guitar \t\t A woman watches TV \t\t Score: 0.1759\n",
            "The new movie is so great \t\t Do you like pizza? \t\t Score: 0.1615\n",
            "The cat plays in the garden \t\t A woman watches TV \t\t Score: 0.1521\n",
            "The cat sits outside \t\t The new movie is awesome \t\t Score: 0.1475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMICTd31sD8_"
      },
      "source": [
        "# Paraphrase Mining"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maPi7Vflnzx8",
        "outputId": "6e9c90be-03fa-44cc-f301-7f295079f5af"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer, util\r\n",
        "\r\n",
        "model = SentenceTransformer('distilbert-base-nli-stsb-mean-tokens')\r\n",
        "\r\n",
        "# Single list of sentences - Possible tens of thousands of sentences\r\n",
        "sentences = ['The cat sits outside',\r\n",
        "             'A man is playing guitar',\r\n",
        "             'I love pasta',\r\n",
        "             'The new movie is awesome',\r\n",
        "             'The cat plays in the garden',\r\n",
        "             'A woman watches TV',\r\n",
        "             'The new movie is so great',\r\n",
        "             'Do you like pizza?']\r\n",
        "\r\n",
        "paraphrases = util.paraphrase_mining(model, sentences)\r\n",
        "\r\n",
        "for paraphrase in paraphrases[0:10]:\r\n",
        "    score, i, j = paraphrase\r\n",
        "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[i], sentences[j], score))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 245M/245M [00:17<00:00, 14.0MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "The new movie is awesome \t\t The new movie is so great \t\t Score: 0.9816\n",
            "The cat sits outside \t\t The cat plays in the garden \t\t Score: 0.6247\n",
            "I love pasta \t\t The new movie is so great \t\t Score: 0.2605\n",
            "I love pasta \t\t The new movie is awesome \t\t Score: 0.2526\n",
            "I love pasta \t\t The cat plays in the garden \t\t Score: 0.2455\n",
            "I love pasta \t\t Do you like pizza? \t\t Score: 0.1997\n",
            "The cat sits outside \t\t A woman watches TV \t\t Score: 0.1837\n",
            "The cat plays in the garden \t\t A woman watches TV \t\t Score: 0.1760\n",
            "A man is playing guitar \t\t Do you like pizza? \t\t Score: 0.1080\n",
            "A woman watches TV \t\t The new movie is so great \t\t Score: 0.1008\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQkyNvPfk-8k"
      },
      "source": [
        "## Semantic Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxxx0BresIJC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4bbc8f2-1234-4f70-b561-04732e48051e"
      },
      "source": [
        "\r\n",
        "\"\"\"This is a simple application for sentence embeddings: semantic search\r\n",
        "\r\n",
        "We have a corpus with various sentences. Then, for a given query sentence,\r\n",
        "we want to find the most similar sentence in this corpus.\r\n",
        "\r\n",
        "This script outputs for various queries the top 5 most similar sentences in the corpus.\r\n",
        "\"\"\"\r\n",
        "from sentence_transformers import SentenceTransformer, util\r\n",
        "import torch\r\n",
        "\r\n",
        "embedder = SentenceTransformer('paraphrase-distilroberta-base-v1')\r\n",
        "\r\n",
        "# Corpus with example sentences\r\n",
        "corpus = ['A man is eating food.',\r\n",
        "          'A man is eating a piece of bread.',\r\n",
        "          'The girl is carrying a baby.',\r\n",
        "          'A man is riding a horse.',\r\n",
        "          'A woman is playing violin.',\r\n",
        "          'Two men pushed carts through the woods.',\r\n",
        "          'A man is riding a white horse on an enclosed ground.',\r\n",
        "          'A monkey is playing drums.',\r\n",
        "          'A cheetah is running behind its prey.'\r\n",
        "          ]\r\n",
        "corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)\r\n",
        "\r\n",
        "# Query sentences:\r\n",
        "queries = ['A man is eating pasta.', 'Someone in a gorilla costume is playing a set of drums.', 'A cheetah chases prey on across a field.']\r\n",
        "\r\n",
        "\r\n",
        "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\r\n",
        "top_k = min(5, len(corpus))\r\n",
        "for query in queries:\r\n",
        "    query_embedding = embedder.encode(query, convert_to_tensor=True)\r\n",
        "\r\n",
        "    # We use cosine-similarity and torch.topk to find the highest 5 scores\r\n",
        "    cos_scores = util.pytorch_cos_sim(query_embedding, corpus_embeddings)[0]\r\n",
        "    top_results = torch.topk(cos_scores, k=top_k)\r\n",
        "\r\n",
        "    print(\"\\n\\n======================\\n\\n\")\r\n",
        "    print(\"Query:\", query)\r\n",
        "    print(\"\\nTop 5 most similar sentences in corpus:\")\r\n",
        "\r\n",
        "    for score, idx in zip(top_results[0], top_results[1]):\r\n",
        "        print(corpus[idx], \"(Score: {:.4f})\".format(score))\r\n",
        "\r\n",
        "    \"\"\"\r\n",
        "    # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk\r\n",
        "    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)\r\n",
        "    hits = hits[0]      #Get the hits for the first query\r\n",
        "    for hit in hits:\r\n",
        "        print(corpus[hit['corpus_id']], \"(Score: {:.4f})\".format(hit['score']))\r\n",
        "    \"\"\""
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: A man is eating pasta.\n",
            "\n",
            "Top 5 most similar sentences in corpus:\n",
            "A man is eating food. (Score: 0.7096)\n",
            "A man is eating a piece of bread. (Score: 0.6074)\n",
            "A man is riding a horse. (Score: 0.3360)\n",
            "A man is riding a white horse on an enclosed ground. (Score: 0.3069)\n",
            "A woman is playing violin. (Score: 0.2378)\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: Someone in a gorilla costume is playing a set of drums.\n",
            "\n",
            "Top 5 most similar sentences in corpus:\n",
            "A monkey is playing drums. (Score: 0.6842)\n",
            "A woman is playing violin. (Score: 0.3762)\n",
            "A man is riding a horse. (Score: 0.3079)\n",
            "A cheetah is running behind its prey. (Score: 0.2760)\n",
            "A man is eating a piece of bread. (Score: 0.2495)\n",
            "\n",
            "\n",
            "======================\n",
            "\n",
            "\n",
            "Query: A cheetah chases prey on across a field.\n",
            "\n",
            "Top 5 most similar sentences in corpus:\n",
            "A cheetah is running behind its prey. (Score: 0.7814)\n",
            "A monkey is playing drums. (Score: 0.2824)\n",
            "A man is riding a white horse on an enclosed ground. (Score: 0.2208)\n",
            "A man is riding a horse. (Score: 0.2017)\n",
            "A man is eating food. (Score: 0.1886)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhIxNPUa4VWX"
      },
      "source": [
        "## Elastic Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "a5dcvDNBlO0Q",
        "outputId": "b317a9d1-1d32-4d58-d478-40d460203f21"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer, util\r\n",
        "import os\r\n",
        "from elasticsearch import Elasticsearch, helpers\r\n",
        "import csv\r\n",
        "import time\r\n",
        "import tqdm.autonotebook\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "es = Elasticsearch()\r\n",
        "\r\n",
        "model = SentenceTransformer('quora-distilbert-multilingual')\r\n",
        "\r\n",
        "url = \"http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv\"\r\n",
        "dataset_path = \"quora_duplicate_questions.tsv\"\r\n",
        "max_corpus_size = 100000\r\n",
        "\r\n",
        "#Download dataset if needed\r\n",
        "if not os.path.exists(dataset_path):\r\n",
        "    print(\"Download dataset\")\r\n",
        "    util.http_get(url, dataset_path)\r\n",
        "\r\n",
        "#Get all unique sentences from the file\r\n",
        "all_questions = {}\r\n",
        "with open(dataset_path, encoding='utf8') as fIn:\r\n",
        "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\r\n",
        "    for row in reader:\r\n",
        "        all_questions[row['qid1']] = row['question1']\r\n",
        "        if len(all_questions) >= max_corpus_size:\r\n",
        "            break\r\n",
        "\r\n",
        "        all_questions[row['qid2']] = row['question2']\r\n",
        "        if len(all_questions) >= max_corpus_size:\r\n",
        "            break\r\n",
        "\r\n",
        "qids = list(all_questions.keys())\r\n",
        "questions = [all_questions[qid] for qid in qids]\r\n",
        "\r\n",
        "#Index data, if the index does not exists\r\n",
        "if not es.indices.exists(index=\"quora\"):\r\n",
        "    try:\r\n",
        "        es_index = {\r\n",
        "            \"mappings\": {\r\n",
        "              \"properties\": {\r\n",
        "                \"question\": {\r\n",
        "                  \"type\": \"text\"\r\n",
        "                },\r\n",
        "                \"question_vector\": {\r\n",
        "                  \"type\": \"dense_vector\",\r\n",
        "                  \"dims\": 768\r\n",
        "                }\r\n",
        "              }\r\n",
        "            }\r\n",
        "        }\r\n",
        "\r\n",
        "        es.indices.create(index='quora', body=es_index, ignore=[400])\r\n",
        "        chunk_size = 500\r\n",
        "        print(\"Index data (you can stop it by pressing Ctrl+C once):\")\r\n",
        "        with tqdm.tqdm(total=len(qids)) as pbar:\r\n",
        "            for start_idx in range(0, len(qids), chunk_size):\r\n",
        "                end_idx = start_idx+chunk_size\r\n",
        "\r\n",
        "                embeddings = model.encode(questions[start_idx:end_idx], show_progress_bar=False)\r\n",
        "                bulk_data = []\r\n",
        "                for qid, question, embedding in zip(qids[start_idx:end_idx], questions[start_idx:end_idx], embeddings):\r\n",
        "                    bulk_data.append({\r\n",
        "                            \"_index\": 'quora',\r\n",
        "                            \"_id\": qid,\r\n",
        "                            \"_source\": {\r\n",
        "                                \"question\": question,\r\n",
        "                                \"question_vector\": embedding\r\n",
        "                            }\r\n",
        "                        })\r\n",
        "\r\n",
        "                helpers.bulk(es, bulk_data)\r\n",
        "                pbar.update(chunk_size)\r\n",
        "\r\n",
        "    except:\r\n",
        "        print(\"During index an exception occured. Continue\\n\\n\")\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "#Interactive search queries\r\n",
        "while True:\r\n",
        "    inp_question = input(\"Please enter a question: \")\r\n",
        "\r\n",
        "    encode_start_time = time.time()\r\n",
        "    question_embedding = model.encode(inp_question)\r\n",
        "    encode_end_time = time.time()\r\n",
        "\r\n",
        "    #Lexical search\r\n",
        "    bm25 = es.search(index=\"quora\", body={\"query\": {\"match\": {\"question\": inp_question }}})\r\n",
        "\r\n",
        "    #Sematic search\r\n",
        "    sem_search = es.search(index=\"quora\", body={\r\n",
        "          \"query\": {\r\n",
        "            \"script_score\": {\r\n",
        "              \"query\": {\r\n",
        "                \"match_all\": {}\r\n",
        "              },\r\n",
        "              \"script\": {\r\n",
        "                \"source\": \"cosineSimilarity(params.queryVector, doc['question_vector']) + 1.0\",\r\n",
        "                \"params\": {\r\n",
        "                  \"queryVector\": question_embedding\r\n",
        "                }\r\n",
        "              }\r\n",
        "            }\r\n",
        "          }\r\n",
        "        })\r\n",
        "\r\n",
        "    print(\"Input question:\", inp_question)\r\n",
        "    print(\"Computing the embedding took {:.3f} seconds, BM25 search took {:.3f} seconds, semantic search with ES took {:.3f} seconds\".format(encode_end_time-encode_start_time, bm25['took']/1000, sem_search['took']/1000))\r\n",
        "\r\n",
        "    print(\"BM25 results:\")\r\n",
        "    for hit in bm25['hits']['hits'][0:5]:\r\n",
        "        print(\"\\t{}\".format(hit['_source']['question']))\r\n",
        "\r\n",
        "    print(\"\\nSemantic Search results:\")\r\n",
        "    for hit in sem_search['hits']['hits'][0:5]:\r\n",
        "        print(\"\\t{}\".format(hit['_source']['question']))\r\n",
        "\r\n",
        "    print(\"\\n\\n========\\n\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ConnectionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 159\u001b[0;31m                 (self._dns_host, self.port), self.timeout, **extra_kw)\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/elasticsearch/connection/http_urllib3.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[1;32m    249\u001b[0m             response = self.pool.urlopen(\n\u001b[0;32m--> 250\u001b[0;31m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRetry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest_headers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    637\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[0;32m--> 638\u001b[0;31m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[1;32m    639\u001b[0m             \u001b[0mretries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# Disabled, indicate to re-raise the error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/packages/six.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(tp, value, tb)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1280\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1281\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1326\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1275\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1276\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1041\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    979\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m             raise NewConnectionError(\n\u001b[0;32m--> 168\u001b[0;31m                 self, \"Failed to establish a new connection: %s\" % e)\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7efd2b8e1f60>: Failed to establish a new connection: [Errno 111] Connection refused",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-523f61858595>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;31m#Index data, if the index does not exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"quora\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         es_index = {\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/elasticsearch/client/utils.py\u001b[0m in \u001b[0;36m_wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                     \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/elasticsearch/client/indices.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(self, index, params, headers)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         return self.transport.perform_request(\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;34m\"HEAD\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_make_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         )\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/elasticsearch/transport.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[1;32m    411\u001b[0m                     \u001b[0;31m# raise exception on last retry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/elasticsearch/transport.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, headers, params, body)\u001b[0m\n\u001b[1;32m    386\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                     \u001b[0mignore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m                 )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/elasticsearch/connection/http_urllib3.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, url, params, body, timeout, ignore, headers)\u001b[0m\n\u001b[1;32m    260\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReadTimeoutError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mConnectionTimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"TIMEOUT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"N/A\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;31m# raise warnings if any from the 'Warnings' header.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionError\u001b[0m: ConnectionError(<urllib3.connection.HTTPConnection object at 0x7efd2b8e1f60>: Failed to establish a new connection: [Errno 111] Connection refused) caused by: NewConnectionError(<urllib3.connection.HTTPConnection object at 0x7efd2b8e1f60>: Failed to establish a new connection: [Errno 111] Connection refused)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5_kEgSR4lxb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}