{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20News_group_Text_classification_NLP",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOqLtp1mpKl9rDlyLLkzmq2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shailavij/NLP/blob/master/20News_group_Text_classification_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOicUqziI8Wu",
        "colab_type": "text"
      },
      "source": [
        "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of my knowledge, it was originally collected by Ken Lang, probably for his Newsweeder: Learning to filter netnews paper, though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qoq3XTDyJC1w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "7dbdbe4f-1112-4c8c-99d1-ac5a92d02eaa"
      },
      "source": [
        "# Loading the Dataset\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "twen_train=fetch_20newsgroups(subset='train',shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading 20news dataset. This may take a few minutes.\n",
            "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cH7EWkkoJRBt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "437a4334-ec8c-4729-cd33-3ea95af94079"
      },
      "source": [
        "# Print all categories\n",
        "twen_train.target_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'comp.windows.x',\n",
              " 'misc.forsale',\n",
              " 'rec.autos',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.sport.hockey',\n",
              " 'sci.crypt',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.space',\n",
              " 'soc.religion.christian',\n",
              " 'talk.politics.guns',\n",
              " 'talk.politics.mideast',\n",
              " 'talk.politics.misc',\n",
              " 'talk.religion.misc']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tyBaNim_J0B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d7ffcd9a-2cea-49b5-e7f4-c98d53d77ac6"
      },
      "source": [
        "twen_train.target[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7,  4,  4,  1, 14, 16, 13,  3,  2,  4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdWl8Mk0Jt_a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "e1515a1f-6cdc-4672-fdfe-2d49a3f59acc"
      },
      "source": [
        "twen_train.data[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21P2Fhw7KCou",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "8c760fe1-e633-4f69-d3b7-aed6a0ec4fd7"
      },
      "source": [
        "# To print first line of first data file\n",
        "print(\"\\n\".join(twen_train.data[0].split(\"\\n\")[:4]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "From: lerxst@wam.umd.edu (where's my thing)\n",
            "Subject: WHAT car is this!?\n",
            "Nntp-Posting-Host: rac3.wam.umd.edu\n",
            "Organization: University of Maryland, College Park\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0o2yWUUyKZGZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "adba3e85-1244-413b-e8ca-86215481e2f8"
      },
      "source": [
        "# Extract features from text files\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(twen_train.data)\n",
        "X_train_counts.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314, 130107)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcK6NvteN_ek",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "a34e3539-f8e5-4dc8-e368-6fccb158d96b"
      },
      "source": [
        "# Sample example for countvectorise\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = ['This is the first document.',\n",
        "'This document is the second document.',\n",
        "'And this is the third one.',\n",
        "'Is this the first document?',\n",
        "]\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "print(vectorizer.get_feature_names())\n",
        "#'and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
        "print(\"X_shape\",X.shape)\n",
        "print(X.toarray())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
            "X_shape (4, 9)\n",
            "[[0 1 1 1 0 0 1 0 1]\n",
            " [0 2 0 1 0 1 1 0 1]\n",
            " [1 0 0 1 1 0 1 1 1]\n",
            " [0 1 1 1 0 0 1 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmgTVuNxTOy1",
        "colab_type": "text"
      },
      "source": [
        "Here by doing ‘count_vect.fit_transform(twent_train.data)’, we are learning the vocabulary dictionary and it returns a Document-Term matrix. [n_samples, n_features]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvctwpbQTQCt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1f59b9d9-e3e5-41ce-d20a-0a842238c2c2"
      },
      "source": [
        "# TF-IDF\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "X_train_tfidf.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314, 130107)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdqeBYr9UNeD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Machine learning\n",
        "# Training Navies bayes classifier on training data\n",
        "\n",
        "from sklearn.naive_bayes import  MultinomialNB\n",
        "clf= MultinomialNB().fit(X_train_tfidf,twen_train.target)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHaEqaUAUinM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Building a pipeline: We can write less code and do all of the above, by building a pipeline as follows:\n",
        "# The names ‘vect’ , ‘tfidf’ and ‘clf’ are arbitrary but will be used later.\n",
        "# We will be using the 'text_clf' going forward.\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "text_clf = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()), ('clf', MultinomialNB())])\n",
        "\n",
        "text_clf = text_clf.fit(twen_train.data, twen_train.target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwdhgR2pWvta",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "15fcc6f0-2ca1-44f5-a220-2ad6cb00558c"
      },
      "source": [
        "\n",
        "# Performance of NB Classifier\n",
        "import numpy as np\n",
        "twenty_test = fetch_20newsgroups(subset='test', shuffle=True)\n",
        "predicted = text_clf.predict(twenty_test.data)\n",
        "np.mean(predicted == twenty_test.target)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7738980350504514"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIx_6NdvXVJO",
        "colab_type": "text"
      },
      "source": [
        "Using Navies Bayes classifier, accuray is 77%\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9xKpdjfXE_g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "22f192f9-b81d-4e46-a09c-347140026438"
      },
      "source": [
        "\n",
        "# Training Support Vector Machines - SVM and calculating its performance\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "text_clf_svm = Pipeline([('vect', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
        "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=5, random_state=42))])\n",
        "\n",
        "text_clf_svm = text_clf_svm.fit(twen_train.data, twen_train.target)\n",
        "predicted_svm = text_clf_svm.predict(twenty_test.data)\n",
        "np.mean(predicted_svm == twenty_test.target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_stochastic_gradient.py:557: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
            "  ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8248805098247477"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahBUZH0_YJzs",
        "colab_type": "text"
      },
      "source": [
        "Using SVM , We got Accuracy around 82%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4I_UA3DqXvhe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Grid Search\n",
        "# Here, we are creating a list of parameters for which we would like to do performance tuning. \n",
        "# All the parameters name start with the classifier name (remember the arbitrary name we gave). \n",
        "# E.g. vect__ngram_range; here we are telling to use unigram and bigrams and choose the one which is optimal.\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters = {'vect__ngram_range': [(1, 1), (1, 2)], 'tfidf__use_idf': (True, False), 'clf__alpha': (1e-2, 1e-3)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpFSS4pkYs8O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "outputId": "f19f62ae-648f-4f90-f193-55a8fc083ce8"
      },
      "source": [
        "# Next, we create an instance of the grid search by passing the classifier, parameters \n",
        "# and n_jobs=-1 which tells to use multiple cores from user machine.\n",
        "\n",
        "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
        "gs_clf = gs_clf.fit(twen_train.data, twen_train.target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:691: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-37deff242e76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgs_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_clf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgs_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtwen_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtwen_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    708\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 710\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1149\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1150\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1151\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    687\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 689\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    538\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    539\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    541\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzR8grD5ZLNn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# NLTK\n",
        "# Removing stop words\n",
        "from sklearn.pipeline import Pipeline\n",
        "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')), ('tfidf', TfidfTransformer()), \n",
        "                     ('clf', MultinomialNB())])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZXxnISWfnS9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 318
        },
        "outputId": "f3a399e8-261e-44c9-d6f7-e6dcd3d743f4"
      },
      "source": [
        "pip install -U nltk\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
            "\u001b[K     |████████████████████████████████| 1.4MB 4.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.6/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from nltk) (0.15.1)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.6/dist-packages (from nltk) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from nltk) (4.41.1)\n",
            "Building wheels for collected packages: nltk\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-cp36-none-any.whl size=1434676 sha256=a01d18fa6827ea8ae6aea5fd2571b0579209a3e69cd2684d5ea4f6e3dc58196d\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
            "Successfully built nltk\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qe0HCbzrhBZR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "f7c6997a-671b-4d8a-bebc-33b39290394c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J2_Zh4yf3-r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d998692c-1d81-4bbb-9655-ff79b6ca89a6"
      },
      "source": [
        "# Stemming Code\n",
        "\n",
        "\n",
        "from nltk.corpus import stopwords \n",
        "#nltk.download()\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "stemmer = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
        "\n",
        "class StemmedCountVectorizer(CountVectorizer):\n",
        "    def build_analyzer(self):\n",
        "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
        "        return lambda doc: ([stemmer.stem(w) for w in analyzer(doc)])\n",
        "    \n",
        "stemmed_count_vect = StemmedCountVectorizer(stop_words='english')\n",
        "\n",
        "text_mnb_stemmed = Pipeline([('vect', stemmed_count_vect), ('tfidf', TfidfTransformer()), \n",
        "                             ('mnb', MultinomialNB(fit_prior=False))])\n",
        "\n",
        "text_mnb_stemmed = text_mnb_stemmed.fit(twen_train.data, twen_train.target)\n",
        "\n",
        "predicted_mnb_stemmed = text_mnb_stemmed.predict(twenty_test.data)\n",
        "\n",
        "np.mean(predicted_mnb_stemmed == twenty_test.target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8167817312798725"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Jf_XQNJgRiJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cbeb71e4-3f05-4077-a99e-35d544417cf4"
      },
      "source": [
        "predicted_mnb_stemmed \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7,  1,  0, ...,  9,  3, 15])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coxla1EphYQH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "d0e7f566-4017-4602-c5a7-8384ade39353"
      },
      "source": [
        "twenty_test.data[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['From: v064mb9k@ubvmsd.cc.buffalo.edu (NEIL B. GANDLER)\\nSubject: Need info on 88-89 Bonneville\\nOrganization: University at Buffalo\\nLines: 10\\nNews-Software: VAX/VMS VNEWS 1.41\\nNntp-Posting-Host: ubvmsd.cc.buffalo.edu\\n\\n\\n I am a little confused on all of the models of the 88-89 bonnevilles.\\nI have heard of the LE SE LSE SSE SSEI. Could someone tell me the\\ndifferences are far as features or performance. I am also curious to\\nknow what the book value is for prefereably the 89 model. And how much\\nless than book value can you usually get them for. In other words how\\nmuch are they in demand this time of year. I have heard that the mid-spring\\nearly summer is the best time to buy.\\n\\n\\t\\t\\tNeil Gandler\\n',\n",
              " 'From: Rick Miller <rick@ee.uwm.edu>\\nSubject: X-Face?\\nOrganization: Just me.\\nLines: 17\\nDistribution: world\\nNNTP-Posting-Host: 129.89.2.33\\nSummary: Go ahead... swamp me.  <EEP!>\\n\\nI\\'m not familiar at all with the format of these \"X-Face:\" thingies, but\\nafter seeing them in some folks\\' headers, I\\'ve *got* to *see* them (and\\nmaybe make one of my own)!\\n\\nI\\'ve got \"dpg-view\" on my Linux box (which displays \"uncompressed X-Faces\")\\nand I\\'ve managed to compile [un]compface too... but now that I\\'m *looking*\\nfor them, I can\\'t seem to find any X-Face:\\'s in anyones news headers!  :-(\\n\\nCould you, would you, please send me your \"X-Face:\" header?\\n\\nI *know* I\\'ll probably get a little swamped, but I can handle it.\\n\\n\\t...I hope.\\n\\nRick Miller  <rick@ee.uwm.edu> | <ricxjo@discus.mil.wi.us>   Ricxjo Muelisto\\nSend a postcard, get one back! | Enposxtigu bildkarton kaj vi ricevos alion!\\n          RICK MILLER // 16203 WOODS // MUSKEGO, WIS. 53150 // USA\\n',\n",
              " 'From: mathew <mathew@mantis.co.uk>\\nSubject: Re: STRONG & weak Atheism\\nOrganization: Mantis Consultants, Cambridge. UK.\\nX-Newsreader: rusnews v1.02\\nLines: 9\\n\\nacooper@mac.cc.macalstr.edu (Turin Turambar, ME Department of Utter Misery) writes:\\n> Did that FAQ ever got modified to re-define strong atheists as not those who\\n> assert the nonexistence of God, but as those who assert that they BELIEVE in \\n> the nonexistence of God?\\n\\nIn a word, yes.\\n\\n\\nmathew\\n',\n",
              " 'From: bakken@cs.arizona.edu (Dave Bakken)\\nSubject: Re: Saudi clergy condemns debut of human rights group!\\nKeywords: international, non-usa government, government, civil rights, \\tsocial issues, politics\\nOrganization: U of Arizona CS Dept, Tucson\\nLines: 101\\n\\nIn article <benali.737307554@alcor> benali@alcor.concordia.ca ( ILYESS B. BDIRA ) writes:\\n>It looks like Ben Baz\\'s mind and heart are also blind, not only his eyes.\\n>I used to respect him, today I lost the minimal amount of respect that\\n>I struggled to keep for him.\\n>To All Muslim netters: This is the same guy who gave a \"Fatwah\" that\\n>Saudi Arabia can be used by the United Ststes to attack Iraq . \\n\\nThey were attacking the Iraqis to drive them out of Kuwait,\\na country whose citizens have close blood and business ties\\nto Saudi citizens.  And me thinks if the US had not helped out\\nthe Iraqis would have swallowed Saudi Arabia, too (or at \\nleast the eastern oilfields).  And no Muslim country was doing\\nmuch of anything to help liberate Kuwait and protect Saudi\\nArabia; indeed, in some masses of citizens were demonstrating\\nin favor of that butcher Saddam (who killed lotsa Muslims),\\njust because he was killing, raping, and looting relatively\\nrich Muslims and also thumbing his nose at the West.\\n\\nSo how would have *you* defended Saudi Arabia and rolled\\nback the Iraqi invasion, were you in charge of Saudi Arabia???\\n\\n>Fatwah is as legitimate as this one. With that kind of \"Clergy\", it might\\n>be an Islamic duty to separate religion and politics, if religion\\n>means \"official Clergy\".\\n\\nI think that it is a very good idea to not have governments have an\\nofficial religion (de facto or de jure), because with human nature\\nlike it is, the ambitious and not the pious will always be the\\nones who rise to power.  There are just too many people in this\\nworld (or any country) for the citizens to really know if a \\nleader is really devout or if he is just a slick operator.\\n\\n>\\n>  \\tCAIRO, Egypt (UPI) -- The Cairo-based Arab Organization for Human\\n>  Rights (AOHR) Thursday welcomed the establishement last week of the\\n>  Committee for Defense of Legal Rights in Saudi Arabia and said it was\\n>  necessary to have such groups operating in all Arab countries.\\n\\nYou make it sound like these guys are angels, Ilyess.  (In your\\nclarinet posting you edited out some stuff; was it the following???)\\nFriday\\'s New York Times reported that this group definitely is\\nmore conservative than even Sheikh Baz and his followers (who\\nthink that the House of Saud does not rule the country conservatively\\nenough).  The NYT reported that, besides complaining that the\\ngovernment was not conservative enough, they have:\\n\\n\\t- asserted that the (approx. 500,000) Shiites in the Kingdom\\n\\t  are apostates, a charge that under Saudi (and Islamic) law\\n\\t  brings the death penalty.  \\n\\n\\t  Diplomatic guy (Sheikh bin Jibrin), isn\\'t he Ilyess?\\n\\n\\t- called for severe punishment of the 40 or so women who\\n\\t  drove in public a while back to protest the ban on\\n\\t  women driving.  The guy from the group who said this,\\n\\t  Abdelhamoud al-Toweijri, said that these women should\\n\\t  be fired from their jobs, jailed, and branded as\\n\\t  prostitutes.\\n\\n\\t  Is this what you want to see happen, Ilyess?  I\\'ve\\n\\t  heard many Muslims say that the ban on women driving\\n\\t  has no basis in the Qur\\'an, the ahadith, etc.\\n\\t  Yet these folks not only like the ban, they want\\n\\t  these women falsely called prostitutes?  \\n\\n\\t  If I were you, I\\'d choose my heroes wisely,\\n\\t  Ilyess, not just reflexively rally behind\\n\\t  anyone who hates anyone you hate.\\n\\n\\t- say that women should not be allowed to work.\\n\\n\\t- say that TV and radio are too immoral in the Kingdom.\\n\\nNow, the House of Saud is neither my least nor my most favorite government\\non earth; I think they restrict religious and political reedom a lot, among\\nother things.  I just think that the most likely replacements\\nfor them are going to be a lot worse for the citizens of the country.\\nBut I think the House of Saud is feeling the heat lately.  In the\\nlast six months or so I\\'ve read there have been stepped up harassing\\nby the muttawain (religious police---*not* government) of Western women\\nnot fully veiled (something stupid for women to do, IMO, because it\\nsends the wrong signals about your morality).  And I\\'ve read that\\nthey\\'ve cracked down on the few, home-based expartiate religious\\ngatherings, and even posted rewards in (government-owned) newspapers\\noffering money for anyone who turns in a group of expartiates who\\ndare worship in their homes or any other secret place. So the\\ngovernment has grown even more intolerant to try to take some of\\nthe wind out of the sails of the more-conservative opposition.\\nAs unislamic as some of these things are, they\\'re just a small\\ntaste of what would happen if these guys overthrow the House of\\nSaud, like they\\'re trying to in the long run.\\n\\nIs this really what you (and Rached and others in the general\\nwest-is-evil-zionists-rule-hate-west-or-you-are-a-puppet crowd)\\nwant, Ilyess?\\n\\n--\\nDave Bakken\\n==>\"the President is doing a fine job, but the problem is we don\\'t know what\\n    to do with her husband.\" James Carville (Clinton campaign strategist),2/93\\n==>\"Oh, please call Daddy. Mom\\'s far too busy.\"  Chelsea to nurse, CSPAN, 2/93\\n',\n",
              " 'From: livesey@solntze.wpd.sgi.com (Jon Livesey)\\nSubject: Re: After 2000 years, can we say that Christian Morality is\\nOrganization: sgi\\nLines: 22\\nDistribution: world\\nNNTP-Posting-Host: solntze.wpd.sgi.com\\n\\nIn article <1993Apr21.141259.12012@st-andrews.ac.uk>, nrp@st-andrews.ac.uk (Norman R. Paterson) writes:\\n|> In article <1r2m21$8mo@fido.asd.sgi.com> livesey@solntze.wpd.sgi.com (Jon Livesey) writes:\\n|> >In article <1993Apr19.151902.21216@st-andrews.ac.uk>, nrp@st-andrews.ac.uk (Norman R. Paterson) writes:\\n> >Just as well, then, that I\\'m not claiming that my own moral system is\\n> >absolute.\\n> >\\n> >jon.\\n> >\\n> >[list of references stretching from here to Alpha Centauri deleted.]\\n>\\n> Jon-\\n>\\n> [and I thought to impress with my references!]\\n>\\n> Ok, so you don\\'t claim to have an absolute moral system.  Do you claim\\n> to have an objective one?  I\\'ll assume your answer is \"yes,\" apologies\\n> if not.\\n\\nI\\'ve just spent two solid months arguing that no such thing as an\\nobjective moral system exists.\\n\\njon.\\n',\n",
              " 'From: banschbach@vms.ocom.okstate.edu\\nSubject: Re: Candida(yeast) Bloom, Fact or Fiction\\nOrganization: OSU College of Osteopathic Medicine\\nLines: 91\\nNntp-Posting-Host: vms.ocom.okstate.edu\\n\\nIn article <1rp8p1$2d3@usenet.INS.CWRU.Edu>, esd3@po.CWRU.Edu (Elisabeth S. Davidson) writes:\\n> \\n> In a previous article, banschbach@vms.ocom.okstate.edu () says:\\n>>least a few \"enlightened\" physicians practicing in the U.S.  It\\'s really \\n>>too bad that most U.S. medical schools don\\'t cover nutrition because if \\n>>they did, candida would not be viewed as a non-disease by so many in the \\n>>medical profession.\\n> \\n> Case Western Reserve Med School teaches nutrition in its own section as\\n> well as covering it in other sections as they apply (i.e. B12\\n> deficiency in neuro as a cause of neuropathy, B12 deficiency in\\n> hematology as a cause of megaloblastic anemia), yet I sill\\n> hold the viewpoint of mainstream medicine:  candida can cause\\n> mucocutaneous candidiasis, and, in already very sick patients\\n> with damaged immune systems like AIDS and cancer patients,\\n> systemic candida infection.  I think \"The Yeast Connection\" is\\n> a bunch of hooey.  What does this have to do with how well\\n> nutrition is taught, anyway?\\n\\nElisabeth, let\\'s set the record straight for the nth time, I have not read \\n\"The Yeast Connection\".  So anything that I say is not due to brainwashing \\nby this \"hated\" book.  It\\'s okay I guess to hate the book, by why hate me?\\nElisabeth, I\\'m going to quote from Zinsser\\'s Microbiology, 20th Edition.\\nA book that you should be familiar with and not \"hate\". \"Candida species \\ncolonize the mucosal surfaces of all humans during birth or shortly \\nthereafter.  The risk of endogenous infection is clearly ever present.  \\nIndeed, candidiasis occurs worldwide and is the most common systemic \\nmycosis.\"  Neutrophils play the main role in preventing a systemic \\ninfection(candidiasis) so you would have to have a low neutrophil count or \\n\"sick\" neutrophils to see a systemic infection.  Poor diet and persistent \\nparasitic infestation set many third world residents up for candidiasis.\\nYour assessment of candidiasis in the U.S. is correct and I do not dispute \\nit.\\n\\nWhat I posted was a discussion of candida blooms, without systemic \\ninfection.  These blooms would be responsible for local sites of irritation\\n(GI tract, mouth, vagina and sinus cavity).  Knocking down the bacterial \\ncompetition for candida was proposed as a possible trigger for candida \\nblooms.  Let me quote from Zinsser\\'s again: \"However, some factors, such as \\nthe use of a broad-spectrum antibacterial antibiotic, may predispose to \\nboth mucosal and systemic infections\".  I was addressing mucosal infections\\n(I like the term blooms better).  The nutrition course that I teach covers \\nthis effect of antibiotic treatment as well as the \"cure\".  I guess that \\nyour nutrition course does not, too bad.  \\n\\n\\n>>Here is a brief primer on yeast.  Yeast infections, as they are commonly \\n>>called, are not truely caused by yeasts.  The most common organism responsible\\n>>for this type of infection is Candida albicans or Monilia which is actually a \\n>>yeast-like fungus.  \\n> \\n> Well, maybe I\\'m getting picky, but I always thought that a yeast\\n> was one form that a fungus could exist in, the other being the\\n> mold form.  Many fungi can occur as either yeasts or molds, \\n> depending on environment.  Candida exibits what is known as\\n> reverse dimorphism - it exists as a mold in the tissues\\n> but exists as a yeast in the environment.  Should we maybe\\n> call it a mold infection?  a fungus infection?  Maybe we\\n> should say it is caused by a mold-like fungus.\\n>  \\n>> \\n>>Martin Banschbach, Ph.D.\\n>>Professor of Biochemistry and Chairman\\n>>Department of Biochemistry and Microbiology\\n>>OSU College of Osteopathic Medicine\\n>>1111 West 17th St.\\n>>Tulsa, Ok. 74107\\n>>\\n> \\n> You\\'re the chairman of Biochem and Micro and you didn\\'t know \\n> that a yeast is a form of a fungus?  (shudder)\\n> Or maybe you did know, and were oversimplifying?\\n\\nMy, my Elisabeth, do I detect a little of Steve Dyer in you?  If you \\nnoticed my faculty rank, I\\'m a biochemist, not a microbiologist.\\nCandida is classifed as a fungus(according to Zinsser\\'s).  But, as you point \\nout, it displays dimorphism.  It is capable of producing yeast cells, \\npseudohyphae and true hyphae.  Elisabeth, you are probably a microbiologist \\nand that makes a lot of sense to you.  To a biochemist, it\\'s a lot of \\nGreek.  So I called it a yeast-like fungus, go ahead and crucify me.\\n\\nYou know Elisabeth, I still haven\\'t been able to figure out why such a small \\nlittle organism like Candida can bring out so much hostility in people in \\nSci. Med.  And I must admitt that I got sucked into the mud slinging too.\\nI keep hoping that if people will just take the time to think about what \\nI\\'ve said, that it will make sense.  I\\'m not asking anyone here to buy into \\n\"The Yeast Connection\" book because I don\\'t know what\\'s in that book, plain \\nand simple. And to be honest with you, I\\'m beginning to wish that it was never \\nwritten.\\n\\nMarty B.\\n',\n",
              " 'From: PETCH@gvg47.gvg.tek.com (Chuck)\\nSubject: Daily Verse\\nLines: 3\\n\\nDishonest money dwindles away, but he who gathers money little by little makes\\nit grow. \\nProverbs 13:11\\n',\n",
              " 'From: fortmann@superbowl.und.ac.za (Paul Fortmann - PG)\\nSubject: \"The Word Perfect\" EXE file needed\\nOrganization: University Of Natal (Durban)\\nLines: 14\\n\\nA friend of mine managed to get a copy of a computerised Greek and Hebrew \\nLexicon called \"The Word Perfect\" (That is not the word processing \\npackage WordPerfect). However, some one wiped out the EXE file, and she \\nhas not been able to restore it. There are no distributors of the package in \\nSouth Africa. I would appreciate it, if some one could email me the file, or \\nat least tell me where I could get it from. \\n\\nMy email address is\\n\\tfortmann@superbowl.und.ac.za     or\\n\\tfortmann@shrike.und.ac.za\\n \\nMany thanks.\\n\\nIn Him, Paul Fortmann\\n',\n",
              " 'From: kartik@hls.COM (Kartik Chandrasekhar)\\nSubject: Multiple(Not Simultaneous) server connections\\nOrganization: The Internet\\nLines: 13\\nNNTP-Posting-Host: enterpoop.mit.edu\\nTo: xpert@expo.lcs.mit.edu\\n\\nHi,\\n\\n   We have a requirement for dynamically closing and opening\\ndifferent display servers within an X application in a manner such\\nthat at any time there is only one display associated with the client.\\n\\n   Assumming a proper cleanup is done during the transition should\\nwe anticipate any problems.\\n\\n\\nkartik\\nkartik@hls.com\\n\\n',\n",
              " \"From: tmc@spartan.ac.BrockU.CA (Tim Ciceran)\\nSubject: Re: Turning photographic images into thermal print and/or negatives\\nOrganization: Brock University, St. Catharines Ontario\\nX-Newsreader: TIN [version 1.1 PL9]\\nLines: 22\\n\\nJennifer Lynn Urso (ju23+@andrew.cmu.edu) wrote:\\n:  \\n: well, i have lots of experience with scanning in images and altering\\n: them.  as for changing them back into negatives, is that really possible?\\n\\n: (stuff deleted)\\n\\n: jennifer urso:  the oh-so bitter woman of utter blahness(but cheerful\\n: undertones)\\n\\nI use Aldus Photostyler on the PC and I can turn a colour or black and white\\nimage into a negative or turn a negative into a colour or black and white\\nimage.  I don't know how it does it but it works well.  To test it I scanned\\na negative and used Aldus to create a positive.  It looked better than the\\nprint that the film developers gave me.\\n\\n\\n-- \\n\\nTMC\\n(tmc@spartan.ac.BrockU.ca)\\n\\n\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4qMotYHhhIH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "93cf6457-5add-4d3e-fdad-0d60289116ce"
      },
      "source": [
        "twenty_test.target_names[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'comp.windows.x',\n",
              " 'misc.forsale',\n",
              " 'rec.autos',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_yLqBSDd_doB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "52273931-3f60-4bd7-84c9-25b8865518de"
      },
      "source": [
        "twenty_test.target[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7,  5,  0, 17, 19, 13, 15, 15,  5,  1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fneisgy1h8v3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "06e0897e-c1d7-4724-e84b-5135a40db2b5"
      },
      "source": [
        "s1=['From: v064mb9k@ubvmsd.cc.buffalo.edu (NEIL B. GANDLER)\\nSubject: Need info on 88-89 Bonneville\\nOrganization: University at Buffalo\\nLines: 10\\nNews-Software: VAX/VMS VNEWS 1.41\\nNntp-Posting-Host: ubvmsd.cc.buffalo.edu\\n\\n\\n I am a little confused on all of the models of the 88-89 bonnevilles.\\nI have heard of the LE SE LSE SSE SSEI. Could someone tell me the\\ndifferences are far as features or performance. I am also curious to\\nknow what the book value is for prefereably the 89 model. And how much\\nless than book value can you usually get them for. In other words how\\nmuch are they in demand this time of year. I have heard that the mid-spring\\nearly summer is the best time to buy.\\n\\n\\t\\t\\tNeil Gandler\\n']\n",
        "s2=['From: Rick Miller <rick@ee.uwm.edu>\\nSubject: X-Face?\\nOrganization: Just me.\\nLines: 17\\nDistribution: world\\nNNTP-Posting-Host: 129.89.2.33\\nSummary: Go ahead... swamp me.  <EEP!>\\n\\nI\\'m not familiar at all with the format of these \"X-Face:\" thingies, but\\nafter seeing them in some folks\\' headers, I\\'ve *got* to *see* them (and\\nmaybe make one of my own)!\\n\\nI\\'ve got \"dpg-view\" on my Linux box (which displays \"uncompressed X-Faces\")\\nand I\\'ve managed to compile [un]compface too... but now that I\\'m *looking*\\nfor them, I can\\'t seem to find any X-Face:\\'s in anyones news headers!  :-(\\n\\nCould you, would you, please send me your \"X-Face:\" header?\\n\\nI *know* I\\'ll probably get a little swamped, but I can handle it.\\n\\n\\t...I hope.\\n\\nRick Miller  <rick@ee.uwm.edu> | <ricxjo@discus.mil.wi.us>   Ricxjo Muelisto\\nSend a postcard, get one back! | Enposxtigu bildkarton kaj vi ricevos alion!\\n          RICK MILLER // 16203 WOODS // MUSKEGO, WIS. 53150 // USA\\n']\n",
        "\n",
        "seq= text_mnb_stemmed.predict(s2)\n",
        "labels=['alt.atheism',\n",
        " 'comp.graphics',\n",
        " 'comp.os.ms-windows.misc',\n",
        " 'comp.sys.ibm.pc.hardware',\n",
        " 'comp.sys.mac.hardware',\n",
        " 'comp.windows.x',\n",
        " 'misc.forsale',\n",
        " 'rec.autos',\n",
        " 'rec.motorcycles',\n",
        " 'rec.sport.baseball']\n",
        "seq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rlclUORf9enR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}